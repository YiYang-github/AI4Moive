{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADfuDWHBJRy-",
        "outputId": "743c6046-eb6d-45eb-cead-ab683554a61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import pickle\n",
        "import torch\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "from random import sample\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuDOLxwmPUI0",
        "outputId": "49bf5fdc-4676-406c-8c67-1e5364c62648"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers sentencepiece\n",
        "\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "import torch\n",
        "\n",
        "# 设置设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# 初始化 MBART 模型和分词器，并移到设备上\n",
        "model_name = 'facebook/mbart-large-50-many-to-one-mmt'\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "def translate_text(text, source_lang=\"zh_CN\", target_lang=\"en_XX\"):\n",
        "    tokenizer.src_lang = source_lang\n",
        "    # 确保编码后的张量也在正确的设备上\n",
        "    encoded = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    generated_tokens = model.generate(**encoded, forced_bos_token_id=tokenizer.lang_code_to_id[target_lang])\n",
        "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "def process_and_translate_srt(input_path, output_path):\n",
        "    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "        srt_block = []\n",
        "        for line in infile:\n",
        "            if line.strip():\n",
        "                srt_block.append(line)\n",
        "            else:\n",
        "                if srt_block:\n",
        "                    # 处理并翻译srt_block\n",
        "                    for srt_line in srt_block:\n",
        "                        outfile.write(srt_line)\n",
        "                        if '-->' not in srt_line and not srt_line.strip().isdigit():\n",
        "                            # 翻译并写入翻译后的文本\n",
        "                            translated_line = translate_text(srt_line.strip())\n",
        "                            print(translated_line)\n",
        "                            outfile.write(translated_line + '\\n')\n",
        "                    outfile.write('\\n')\n",
        "                    srt_block = []\n",
        "        # 处理文件最后一个块\n",
        "        if srt_block:\n",
        "            for srt_line in srt_block:\n",
        "                outfile.write(srt_line)\n",
        "                if '-->' not in srt_line and not srt_line.strip().isdigit():\n",
        "                    translated_line = translate_text(srt_line.strip())\n",
        "                    outfile.write(translated_line + '\\n')\n",
        "            outfile.write('\\n')\n",
        "\n",
        "input_srt_path = '/content/drive/MyDrive/users/yangyi/Moive/Bilingual.srt'\n",
        "output_srt_path = '/content/drive/MyDrive/users/yangyi/Moive/Bilingual_Translated.srt'\n",
        "\n",
        "process_and_translate_srt(input_srt_path, output_srt_path)"
      ],
      "metadata": {
        "id": "zDMqtqYKJtye"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}